vals_n <- vals[[1]][2]
vals_n <- strsplit(vals_n, " ")
size_f <- vals_n[[1]][grep("size", vals_n[[1]] )]
toc_f <- vals_n[[1]][grep("toc", vals_n[[1]])]
if (length(size_f) == 0){
} else {
size <- eval(parse(text = size_f))
}
if (length(toc) == 0) {
} else {
toc <- eval(parse(text = toc_f))
}
sourceSize <- size
sourceToc <- toc
element
size <- t[,3]
toc <- t[,4]
targetSize <- eval(parse(text = size_f))
targetToc <- eval(parse(text = toc_f))
#Drop intercept if there any from coefficients
coefs <- coefficients(fit)[names(coefficients(fit))!="(Intercept)"]
coefs <- as.numeric(coefs)
Yi <- element
Si <- sourceSize
Ti <- sourceToc
Sj <- targetSize
Tj <- targetToc
if (length(coefs) == 3) {
Ss <- coefs[1]# Size slope
Ts <- coefs[2]# Toc slope
TjSj <- coefs[3]# Interaction slope
Corrected <- (Yi - (((Si - Sj) * Ss) + ((Ti - Tj) * Ts) + (((Si * Ti) - (Sj * Tj)) * TjSj)))
if (any(Corrected)<0) { } else {element <- Corrected}
} else if (length(coefs) == 2) {
if (length(size_f) == 0) { #toc and interaction
Ts <- coefs[1]
TjSj <- coefs[2]
Corrected <- (Yi - (((Ti - Tj)*Ts))+ (((Si * Ti) - (Sj * Tj)) * TjSj))
}
else if (length(toc_f) == 0) { #size and interaction
Ss <- coefs[1]
TjSj <- coefs[2]
Corrected <- (Yi - (((Si - Sj)*Ss))+ (((Si * Ti) - (Sj * Tj)) * TjSj))
} else { #size and toc only
Ss <- coefs[1]
Ts <- coefs[2]
Corrected <- (Yi - (((Si - Sj) * Ss) + ((Ti - Tj) * Ts)))
}
} else if (length(coefs) == 1) {
if (length(size_f) == 0) {
Ts <- coefs
Corrected <- (Yi - ((Ti-Tj)*Ts))
} #toc
else {
Ss <- coefs
Corrected <- (Yi - ((Si-Sj)*Ss))
} #size
}
Corrected
datas$Nickel
f$stepV
sqrt(corrected)
sqrt(Corrected)
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
styler:::style_active_file()
runApp('~/ShinyTwit/ShinyTwit.R')
styler:::style_active_file()
install.packages("widyr")
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
install.packages("ggraph")
install.packages("igraph")
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
runApp('~/ShinyTwit/ShinyTwit.R')
rm(list = ls())
source("pcg.R")
source("functions.R")
server <- function(input, output, session) {
options(shiny.maxRequestSize = 70 * 1024^2) # Max csv data limit set to 60 mb
# Page 1 view, maps
output$map <- renderLeaflet({
leaflet() %>%
addProviderTiles(providers$OpenStreetMap) %>%
setView(lng = -4, lat = 52.54, zoom = 3)
})
# Show popup on click
observeEvent(input$map_click, {
click <- input$map_click
text <- paste("Lattitude ", click$lat, "Longtitude ", click$lng)
proxy <- leafletProxy("map")
proxy %>%
clearPopups() %>%
addPopups(click$lng, click$lat, text)
})
# Page 1 view, functions & plots
trends <- reactive({
req(input$map_click)
click <- input$map_click
woeid <- closestTrendLocations(lat = click$lat, long = click$lng)
current_trends <- getTrends(as.numeric(woeid[3]))
current_trends <- as.data.frame(current_trends)
current_trends$trend_date <- Sys.Date()
names(current_trends)[1] <- "Trending"
current_trends
})
# Word cloud
output$p <- renderPlot({
req(trends())
datas <- trends()
x <- datas[, 1]
y <- seq(1, length(x))
y <- y^2
wordcloud(x, sqrt(rev(y)), scale = c(1.2, 0.2), min.freq = 1, colors = brewer.pal(8, "Dark2"), random.order = TRUE, use.r.layout = FALSE, max.words = 200, rot.per = 0.35)
}, height = 350, width = 350)
# Dynamic trends selection & sentiment analysis
output$trends <- DT::renderDT({
req(input$map_click)
#textClean_pairs()
as.data.frame(trends())
})
output$top <- renderUI({
datas <- trends()
selectInput(inputId = "xE", label = "Sentiment & Frequency, N = 300", choices = datas$Trending)
})
Trending_top <- reactive({
req(input$xE)
x <- searchTwitter(input$xE, n = 300, lang = "en")
x <- twListToDF(x)
})
textClean_top <- reactive({
x <- Trending_top()
cleanText(x)
})
sentim_top <- reactive({
x_text <- textClean_top()
x_text.text.corpus <- Corpus(VectorSource(x_text))
x_text.text.corpus <- tm_map(x_text.text.corpus, function(x) removeWords(x, stopwords()))
mysentiment_x <- get_nrc_sentiment((x_text))
})
output$p1_top <- renderPlot({
#print (textClean_pairs())
mysentiment_x <- sentim_top()
Sentimentscores_x <- data.frame(colSums(mysentiment_x[, ]))
names(Sentimentscores_x) <- "Score"
Sentimentscores_x <- cbind("sentiment" = rownames(Sentimentscores_x), Sentimentscores_x)
rownames(Sentimentscores_x) <- NULL
# plotting the sentiments with scores
ggplot(data = Sentimentscores_x, aes(x = sentiment, y = Score)) + geom_bar(aes(fill = sentiment), stat = "identity") +
theme(legend.position = "none") +
xlab("Sentiments") + ylab("scores") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ggtitle(paste("Sentiments of people behind the tweets on", input$xE, sep = " "))
}, height = 330, width = 350)
output$p3_top <- renderPlot({
req(textClean_top())
x_text <- textClean_top()
# convert into corpus type
x_text.text.corpus <- Corpus(VectorSource(x_text))
# clean up by removing stop words
x_text.text.corpus <- tm_map(x_text.text.corpus, function(x) removeWords(x, stopwords()))
dtm <- TermDocumentMatrix(x_text.text.corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
barplot(d[1:10, ]$freq,
las = 2, names.arg = d[1:10, ]$word,
col = "salmon", main = "Most frequent words",
ylab = "Word frequencies" # ,
# theme(axis.text.x = element_text(angle = 90, hjust = 1))
)
}, height = 320, width = 350)
# Page 2 data exploration
# Separate analysis
Trending <- eventReactive(input$looktrending, {
req(input$TweetsN)
req(input$trending)
x <- searchTwitter(input$trending, n = input$TweetsN, lang = "en")
x <- twListToDF(x)
})
output$textClean_pairs <- renderPlot ({
x<- Trending_top()
x$stripped_text <- gsub("http.*","",  x$text)
x$stripped_text <- gsub("https.*","", x$stripped_text)
x_clean <- x %>%
dplyr::select(stripped_text) %>%
unnest_tokens(word, stripped_text)
#
# data ("stop_words")
#
# x_tweet_words <- x_clean %>%
#   anti_join(stop_words)
# #
# # print (x_tweet_words)
# #
# x$stripped_text <- x_tweet_words
#
x_tweets_paired_words <- x %>%
dplyr::select(stripped_text) %>%
unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)
#
x_tweets_separated_words <- x_tweets_paired_words %>%
tidyr::separate(paired_words, c("word1", "word2"), sep = " ")
#
x_tweets_filtered <- x_tweets_separated_words %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#
x_words_counts <- x_tweets_filtered %>%
count(word1, word2, sort = TRUE)
#
# # plot climate change word network
x_words_counts %>%
filter(n >= 24) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = paste("Word Network: Tweets using the hashtag", input$xE, sep = ""),
subtitle = "Text mining twitter data ",
x = "", y = "")
})
textClean <- reactive({
x <- Trending()
cleanText(x)
})
sentim <- reactive({
x_text <- textClean()
# convert into corpus type
x_text.text.corpus <- Corpus(VectorSource(x_text))
# clean up by removing stop words
x_text.text.corpus <- tm_map(x_text.text.corpus, function(x) removeWords(x, stopwords()))
# getting emotions using in-built function
mysentiment_x <- get_nrc_sentiment((x_text))
})
output$p1 <- renderPlot({
mysentiment_x <- sentim()
Sentimentscores_x <- data.frame(colSums(mysentiment_x[, ]))
names(Sentimentscores_x) <- "Score"
Sentimentscores_x <- cbind("sentiment" = rownames(Sentimentscores_x), Sentimentscores_x)
rownames(Sentimentscores_x) <- NULL
# plotting the sentiments with scores
ggplot(data = Sentimentscores_x, aes(x = sentiment, y = Score)) + geom_bar(aes(fill = sentiment), stat = "identity") +
theme(legend.position = "none") +
xlab("Sentiments") + ylab("scores") + ggtitle(paste("Sentiments of people behind the tweets on", input$trending, sep = " "))
}, height = 850, width = 1050)
output$p2 <- renderPlot({
req(textClean())
x_text <- textClean()
# convert into corpus type
x_text.text.corpus <- Corpus(VectorSource(x_text))
# clean up by removing stop words
x_text.text.corpus <- tm_map(x_text.text.corpus, function(x) removeWords(x, stopwords()))
dtm <- TermDocumentMatrix(x_text.text.corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
# dev.cur(width = 1000, height = 1000, unit = "px")
wordcloud(
words = d$word, scale = c(4, .5), freq = sqrt(d$freq), min.freq = 1,
max.words = 200, random.order = TRUE, use.r.layout = FALSE,
rot.per = 0.35,
colors = brewer.pal(8, "Dark2")
)
}, height = 650, width = 750)
output$p3 <- renderPlot({
req(textClean())
x_text <- textClean()
# convert into corpus type
x_text.text.corpus <- Corpus(VectorSource(x_text))
# clean up by removing stop words
x_text.text.corpus <- tm_map(x_text.text.corpus, function(x) removeWords(x, stopwords()))
dtm <- TermDocumentMatrix(x_text.text.corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
barplot(d[1:30, ]$freq,
las = 2, names.arg = d[1:30, ]$word,
col = "lightblue", main = "Most frequent words",
ylab = "Word frequencies"
)
}, height = 750, width = 750)
}
ui <- dashboardPage(
skin = "blue",
dashboardHeader(title = "ShinyTwit"),
## Sidebar content
dashboardSidebar(
sidebarMenu(
menuItem("Trending", tabName = "dataInput", icon = icon("upload")),
menuItem("Search", tabName = "regressions", icon = icon("random")),
menuItem("Analytics", tabName = "mixmod", icon = icon("cubes"))
)
),
dashboardBody( # Body content
tabItems(
tabItem( # First tab content
tabName = "dataInput",
fluidRow(
column(
width = 9,
box(
title = "Twitter Review: ", status = "success", height =
"900", width = "12", solidHeader = T,
tabsetPanel(
tabPanel(
"Location",
box(
width = 12,
leafletOutput("map", height = 750)
)
),
tabPanel(
"Table",
box(
width = 12,
DT::dataTableOutput("trends"), style = "height:700px; overflow-y: scroll;overflow-x: scroll;"
)
)
)
)
),
column(
width = 3, align = "center",
# box(
#   title = "Current Trends: ", status = "warning", height =
#     "400", width = "400", solidHeader = T,
#   plotOutput("p"), style = "height:350px;width:400;"#overflow-y: scroll;overflow-x: scroll;"
# ),
box(
title = "Top in Trends: ", status = "success", height =
"900", width = "400", solidHeader = T,
uiOutput("top"),
tabsetPanel(
tabPanel(
"Sentiment",
column(12,
align = "center",
withSpinner(plotOutput("p1_top")),
withSpinner(plotOutput("p3_top"))
)
),
tabPanel(
"Wordcloud & Pairs",
column(12,
align = "center",
withSpinner(plotOutput("p")),
withSpinner(plotOutput("textClean_pairs"))
)
)
)
)
)
)
),
tabItem( # First tab content
tabName = "regressions",
fluidRow(
column(
width = 3,
box(
title = "Twitter Input: ", status = "success", height =
"1595", width = "12", solidHeader = T,
textInput("trending", "Hashtag", "#Champion", width = 200),
numericInput("TweetsN", "Number of tweets", 5, min = 1, max = 10001, width = 200),
actionButton("looktrending", "Lookup") # updated from July 28
)
),
column(
width = 9,
box(
title = "Twitter Input: ", status = "success", height =
"1595", width = "12", solidHeader = T,
tabsetPanel(
tabPanel(
"Sentiment Plot",
box(
width = 12,
column(12,
align = "center",
withSpinner(plotOutput("p1")), style = "height:800px;width:600;overflow-y: scroll;overflow-x: scroll;"
)
)
),
tabPanel(
"WordCloud",
box(
width = 12,
column(12,
align = "center",
withSpinner(plotOutput("p2")), style = "height:800px;width:600;overflow-y: scroll;overflow-x: scroll;"
)
)
),
tabPanel(
"Word Frequency",
box(
width = 12,
column(12,
align = "center",
withSpinner(plotOutput("p3")), style = "height:800px;width:600;overflow-y: scroll;overflow-x: scroll;"
)
)
)
)
)
)
)
)
)
)
)
# Run the app ----
shinyApp(ui, server)
rm(list = ls())
source("pcg.R")
source("functions.R")
setwd("~/ShinyTwit")
rm(list = ls())
source("pcg.R")
source("functions.R")
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
runApp('ShinyTwit.R')
rm(list = ls())
x <- try(setwd(dirname(rstudioapi::getSourceEditorContext()$path)))
if (x != "try-error") {
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
print("no errors")
} else {
filename <- "collector.R"
filepath <- file.choose() # browse and select your_file.R in the window
dir <- substr(filepath, 1, nchar(filepath) - nchar(filename))
setwd(dir)
}
source('pcg.R')
#Get working directory, check for TwitLib folder
mainDir <- getwd()
subDir <- "TwitLib"
if (file.exists(subDir)){
setwd(file.path(mainDir, subDir))
} else {
dir.create(file.path(mainDir, subDir))
setwd(file.path(mainDir, subDir))
}
csv_hashtags <- 'hashtagsList.csv'
hashtagSearch <- 'blockchain'
if (file.exists(csv_hashtags)){
lib <- read.csv (csv_hashtags)
if (hashtagSearch %in% lib[,1]) {} else {
write.table(hashtagSearch, file = csv_hashtags, sep = ",", append = TRUE, quote = FALSE,
col.names = FALSE, row.names = FALSE)
}
} else {
write.csv (hashtagSearch, file = csv_hashtags, sep = ",", col.names = F, row.names = F)
}
#Create empty library, if it does not exist alredy
l<-list.files ()
hashtagLib <- paste(hashtagSearch, '.csv', sep = "")
l
if (hashtagLib %in% l){
datas <- read.csv (hashtagLib)
} else {
datas <- NULL
}
sleep_for_a_minute <- function() { Sys.sleep(900) }
start_time <- Sys.time()
#sys.time()
c <- 0
while(abs(as.numeric(difftime(start_time, Sys.time(), units = 'secs')))<(2000))
{
dat <- searchTwitter(hashtagSearch, n = 1500,lang = "en")
dat <- twListToDF(dat)
datas <- rbind (datas, dat)
datas <- unique(datas)
c = c+1
print (paste0('Call: ', c))
if (c%%3 == 0) {
sleep_for_a_minute ()
}
}
library ('taskscheduleR)')
library ('taskscheduleR)
library ('taskscheduleR')
library ('taskscheduleR')
install.packages("taskscheduleR")
taskscheduleR:::taskschedulerAddin()
taskscheduleR:::taskschedulerAddin()
taskscheduleR:::taskschedulerAddin()
taskscheduleR:::taskschedulerAddin()
